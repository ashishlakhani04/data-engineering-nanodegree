Glossary

Stream - An unbounded sequence of ordered, immutable data
Stream Processing - Continual calculations performed on one or more Streams
Immutable Data - Data that cannot be changed once it has been created
Event - An immutable fact regarding something that has occurred in our system.
Batch Processing - Scheduled, periodic analysis of one or more groups of related data.
Data Store - A generic place that holds data of some kind, like a message queue or data store
Stream Processing Application - An application which is downstream of one or more data streams and performs some kind of calculation on incoming data, typically producing one or more output data streams
Stream Processing Framework - A set of tools, typically bundled as a library, used to construct a Stream Processing Application
Real-time - In relation to processing, this implies that a piece of data, or an event, is processed almost as soon as it is produced. Strict time-based definitions of real-time are controversial in the industry and vary widely between applications. For example, a Computer Vision application may consider real-time to be 1 millisecond or less, whereas a data engineering team may consider it to be 30 seconds or less. In this class when the term "real-time" is used, the time-frame we have in mind is seconds.
Append-only Log - files in which incoming events are written to the end of the file as they are received
Change Data Capture (CDC) - The process of capturing change events, typically in SQL database systems, in order to accurately communicate and synchronize changes from primary to replica nodes in a clustered system.
Log-Structured Storage - Systems built on Append-Only Logs, in which system data is stored in log format.
Merge (Log Files) - When two or more log files are joined together into a single output log file
Compact (Log Files) - When data from one or more files is deleted, typically based on the age of data
Source (Kafka) - A term sometimes used to refer to Kafka clients which are producing data into Kafka, typically in reference to another data store
Sink (Kafka) - A term sometimes used to refer to Kafka clients which are extracting data from Kafka, typically in reference to another data store
Topic (Kafka) - A logical construct used to organize and segment datasets within Kafka, similar to how SQL databases use tables
Producer (Kafka) - An application which is sending data to one or more Kafka Topics.
Consumer (Kafka) - An application which is receiving data from one or more Kafka Topics.



Batch Processing

Runs on a scheduled basis
May run for a longer period of time and write results to a SQL-like store
May analyze all historical data at once
Typically works with mutable data and data stores



Stream Processing

Runs at whatever frequency events are generated
Typically runs quickly, updating in-memory aggregates
Stream Processing applications may simply emit events themselves, rather than write to an event store
Typically analyzes trends over a limited period of time due to data volume
Typically analyzes immutable data and data stores




Streaming Data Store

May look like a message queue, as is the case with Apache Kafka
May look like a SQL store, as is the case with Apache Cassandra
Responsible for holding all of the immutable event data in the system
Provides guarantee that data is stored ordered according to the time it was produced
Provides guarantee that data is produced to consumers in the order it was received
Provides guarantee that the events it stores are immutable and unchangeable



Stream Processing Application and Framework

Stream Processing applications sit downstream of the data store
Stream Processing applications ingest real-time event data from one or more data streams
Stream Processing applications aggregate, join, and find differences in data from these streams
Common Stream Processing Application Frameworks in use today include:
Confluent KSQL
Kafka Streams
Apache Flink
Apache Samza
Apache Spark Structure Streaming
Faust Python Library



Benefits of Stream Processing

Faster for scenarios where a limited set of recent data is needed
More scalable due to distributed nature of storage
Provides a useful abstraction that decouples applications from each other
Allows one set of data to satisfy many use-cases which may not have been predictable when the dataset was originally created
Built-in ability to replay events and observe exactly what occurred, and in what order, provides more opportunities to recover from error states or dig into how a particular result was arrived at


Append-only logs

Append-only logs are text files in which incoming events are written to the end of the log as they are received.
This simple concept -- of only ever appending, or adding, data to the end of a log file -- is what allows stream processing applications to ensure that events are ordered correctly even at high throughput and scale.
We can take this idea a step farther, and say that in fact, streams are append-only logs.